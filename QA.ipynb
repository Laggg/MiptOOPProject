{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple pipeline for training QA models using simpletransformers wrapper around transformers library implemented by HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import typing\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simpletransformers\n",
    "from simpletransformers.question_answering import QuestionAnsweringModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "?simpletransformers.question_answering.QuestionAnsweringModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAModel(simpletransformers.question_answering.QuestionAnsweringModel):\n",
    "    \"\"\"\n",
    "    A simple wrapper around the QuestionAnsweringModel class from simpletransformers.\n",
    "    By default, it is set to using CPU, not GPU; set use_cuda=True in class initialization to force it to use GPU.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"distilbert\",model_path=\"distilbert-base-uncased-distilled-squad\",use_cuda=True, **args):\n",
    "        \"\"\"\n",
    "        ::param model_name - type of model to use\n",
    "        ::param model_path - path to pretrained model directory or standard \n",
    "                        model specification from here https://huggingface.co/transformers/pretrained_models.html \n",
    "                        or here https://huggingface.co/models\n",
    "        \"\"param use_cuda - whether to use GPUs or not\n",
    "        ::param args - specifies args passed to simpletransformers model. Take a look here https://simpletransformers.ai/docs/usage/\n",
    "        \"\"\"\n",
    "        super(QAModel, self).__init__(model_name,model_path, use_cuda=use_cuda,  args=args)\n",
    "        #self.model_type = model_name\n",
    "        #self.model_path = model_path\n",
    "        #self.args=args\n",
    "        self.skipped = [] # indices of examples that did not fit some conditions on data preparation during .fit() call\n",
    "    \n",
    "    \n",
    "    def fit(self, data:typing.Union[pd.core.frame.DataFrame, pd.core.series.Series], fields: typing.Union[pd.core.indexes.base.Index, np.ndarray, list], context_for_all: typing.Union[str] = \"\", question_for_all: typing.Union[str] = \"\") -> None:\n",
    "        \"\"\"\n",
    "        ::param data - containing questions/answers/contexts or questions/answers or answers\n",
    "        ::param fields -  specifies the names of columns with questions, answers and contexts in df, \n",
    "                            passed as strings in this order.if context of all questions is the same, pass in dataframe with only two columns and specify the context_for_all variable.\n",
    "        ::param context_for_all - should be specified if all questions are asked to the same context\n",
    "        ::param question_for_all - should be specified if one question for all training examples is specified\n",
    "        \"\"\"\n",
    "        if context_for_all != \"\" and question_for_all != \"\":\n",
    "            raise NotImplementedError\n",
    "        self.skipped = [] # free space; \n",
    "        \n",
    "        training_data = []\n",
    "        for i  in range(len(data)):\n",
    "            if context_for_all != \"\" and question_for_all == \"\":\n",
    "                context = context_for_all\n",
    "                question = data.loc[i, fields[0]]\n",
    "                answer = data.loc[i, fields[1]]\n",
    "            elif context_for_all == \"\" and question_for_all != \"\":\n",
    "                context = fields[1]\n",
    "                question = question_for_all\n",
    "                answer = df[i]\n",
    "            else:\n",
    "                context = data.loc[i, fields[2]]\n",
    "                question = data.loc[i, fields[0]]\n",
    "                answer = data.loc[i, fields[1]]\n",
    "\n",
    "            qas = []\n",
    "           \n",
    "                    \n",
    "            answers = []\n",
    "            \n",
    "            if type(answer) != str or type(context) != str or type(question) != str:\n",
    "                self.skipped.append(i)\n",
    "                continue\n",
    "            answer_starts = self._find_all(context, answer)\n",
    "            for answer_start in answer_starts:\n",
    "                answers.append({'answer_start': answer_start, 'text': answer.lower()})\n",
    "                break\n",
    "            qas.append({'question': question, \"id\":hash(question + str(np.random.random())), 'is_impossible': False, 'answers': answers})\n",
    "\n",
    "            training_data.append({'context': context.lower(), 'qas': qas})\n",
    "            \n",
    "        #train_args = {'silent':True, 'evaluate_during_training':False, 'output_dir':\"outputs/\", 'no_cache':True,'cache_dir':\"cache\", \"model_type\":self.model_type}\n",
    "            \n",
    "        self.train_model(training_data)\n",
    "        \n",
    "        \n",
    "    def make_prediction(self, test: typing.Union[pd.core.frame.DataFrame]) -> np.ndarray:\n",
    "        output = []\n",
    "            \n",
    "        for i in range(len(test)):\n",
    "            context = test.iloc[i, 1]\n",
    "            qas = []\n",
    "            question = test.iloc[i, 0]\n",
    "            if type(context) != str or type(question) != str:\n",
    "                print(context, type(context))\n",
    "                print(question, type(question))\n",
    "                continue\n",
    "            answers = []\n",
    "            answers.append({'answer_start': 1000000, 'text': '__None__'})\n",
    "            qas.append({'question': question, \"id\":hash(question + str(np.random.random())), 'is_impossible': False, 'answers': answers})\n",
    "            output.append({'context': context.lower(), 'qas': qas})\n",
    "        output = self.predict(output)\n",
    "            \n",
    "        return list(map(lambda x: x['answer'], output))\n",
    "        \n",
    "    def _find_all(self, input_str, search_str):\n",
    "        l1 = []\n",
    "        length = len(input_str)\n",
    "        index = 0\n",
    "        while index < length:\n",
    "            i = input_str.find(search_str, index)\n",
    "            if i == -1:\n",
    "                return l1\n",
    "            l1.append(i)\n",
    "            index = i + 1\n",
    "        return l1\n",
    "\n",
    "\n",
    "        \n",
    "       \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = QAModel(use_cuda=False, args={'reprocess_input_data': True,\n",
    "                                     'overwrite_output_dir': True,\n",
    "                                     'learning_rate': 5e-5,\n",
    "                                     'num_train_epochs': 3,\n",
    "                                     'max_seq_length': 192,\n",
    "                                     'doc_stride': 64,\n",
    "                                     'fp16': False,\n",
    "                                      \n",
    "                                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-c93c309e7d41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'selected_text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-68-0698ddc34c7a>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, data, fields, context_for_all, question_for_all)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m#train_args = {'silent':True, 'evaluate_during_training':False, 'output_dir':\"outputs/\", 'no_cache':True,'cache_dir':\"cache\", \"model_type\":self.model_type}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.env/lib/python3.7/site-packages/simpletransformers/question_answering/question_answering_model.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, train_data, output_dir, show_running_loss, args, eval_data, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mtrain_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_and_cache_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.env/lib/python3.7/site-packages/simpletransformers/question_answering/question_answering_model.py\u001b[0m in \u001b[0;36mload_and_cache_examples\u001b[0;34m(self, examples, evaluate, no_cache, output_examples)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cache_dir\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"dev\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mevaluate\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.env/lib/python3.7/site-packages/simpletransformers/question_answering/question_answering_utils.py\u001b[0m in \u001b[0;36mget_examples\u001b[0;34m(examples_to_process, is_training, version_2_with_negative)\u001b[0m\n\u001b[1;32m    157\u001b[0m                     \u001b[0;31m# Note that this means for training mode, every example is NOT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                     \u001b[0;31m# guaranteed to be preserved.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                     \u001b[0mactual_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_position\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mend_position\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m                     \u001b[0mcleaned_answer_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhitespace_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_answer_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mactual_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_answer_text\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(data, ['sentiment', 'selected_text', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf outputs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/101 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  1%|          | 1/101 [00:00<00:14,  7.07it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 101/101 [00:00<00:00, 696.72it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "821d24ff77914a20bdfeb41521f90564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['if i were going',\n",
       " '',\n",
       " 'my boss is bullying me',\n",
       " 'leave me alone',\n",
       " 'sons of ****',\n",
       " 'smf - some shameless plugging for the best rangers forum on earth',\n",
       " '2am feedings for the baby are fun when he is all smiles and coos',\n",
       " 'soooo high',\n",
       " 'both of you',\n",
       " 'cooler',\n",
       " 'i`m never gonna get my cake',\n",
       " '',\n",
       " 'my sharpie is running dangerously low on ink',\n",
       " 'i lost my voice.',\n",
       " 'lg env2',\n",
       " 'i am sunburned',\n",
       " 'sigh',\n",
       " 'sick',\n",
       " 'gonna miss every one',\n",
       " 'hes just not that into you',\n",
       " 'oh marly',\n",
       " 'can`t wait to have a dragon pet',\n",
       " 'her family',\n",
       " 'i thought win7',\n",
       " 'smh',\n",
       " 'free fillin` app on my ipod is fun, im addicted',\n",
       " 'i`m sorry',\n",
       " 'no internet access to twit',\n",
       " 'juss came backk from berkeleyy',\n",
       " 'went to sleep',\n",
       " 'heavenly',\n",
       " 'i hope unni will make the audition',\n",
       " 'it says i am obesed well so much for being unhappy for about 10 minutes.',\n",
       " 'cute kids',\n",
       " 'ah',\n",
       " 'tears for fears',\n",
       " 'texas',\n",
       " 'we are really busy today and this coming with with adding tons of new blogs and updates stay tuned',\n",
       " 'i`m soooooo sleeeeepy!!! the last day o` school was today....sniffle',\n",
       " 'a little happy',\n",
       " 'car',\n",
       " 'an avid fan',\n",
       " '',\n",
       " 'hott bad boy look',\n",
       " 'where dear? would love to help convert her vids',\n",
       " 'how old',\n",
       " 'egh blah and boooooooooooo',\n",
       " 'visiting my friendster and facebook',\n",
       " 'i also dont like going shopping',\n",
       " 'i got a new one last week and i`m not thrilled at all with mine.',\n",
       " 'twittersucks.com and connect with other tweeple who hate twitter',\n",
       " 'my third freelesson',\n",
       " 'hm',\n",
       " 'it is ****',\n",
       " 'romance zero is funny',\n",
       " 'morning runner',\n",
       " 'bah',\n",
       " 'later',\n",
       " 'aw. torn ace of hearts #hunchback',\n",
       " 'what fun',\n",
       " 'i lost all my friends',\n",
       " 'haha yes',\n",
       " 'i give in to easily',\n",
       " 'our wine was a red',\n",
       " 'jealous',\n",
       " '',\n",
       " 'he`s a good friend',\n",
       " 'boo it`s gonna soggy and i`m at work',\n",
       " 'chilliin',\n",
       " 'if you know such agent',\n",
       " 'kitchenfire',\n",
       " 'lol',\n",
       " 'i promise to buy you a drink and take rad pics for your fb / blog / flickr',\n",
       " 'summer',\n",
       " 'good',\n",
       " 'ok - i`m out of here for now',\n",
       " 'wow',\n",
       " 'my sources say no',\n",
       " 'i am sooo tired',\n",
       " 'you didn`t even tell me',\n",
       " 'thank yyyyyyyyyoooooooooouuuuu!',\n",
       " 'lucky kid',\n",
       " 'fell asleep waiting for my ride',\n",
       " 'flu',\n",
       " 'sorry guys',\n",
       " 'enjoy the holiday',\n",
       " 'i`m in essex',\n",
       " 'his snoring is so annoying n it keeps me from sleeping (like right now, lol) but i honestly wud miss it if it eva left i love him',\n",
       " 'i miss you bby wish you were going tomorrow to make me do good.',\n",
       " 'beta testing',\n",
       " 'sweeeeet - san fran is awesome!!!! love it there',\n",
       " 'mounce',\n",
       " 'twitter',\n",
       " 'gonna be so tired',\n",
       " 'ice cream and then getting ready for graduation',\n",
       " 'happy mothers day',\n",
       " 'she`s prolly freaked',\n",
       " 'hemp cloth',\n",
       " 'nighty night',\n",
       " 'none 3d - the baddie`s the best',\n",
       " '4am']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.make_prediction(data.loc[:100, [ 'sentiment','text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
